{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fake Review Detection - Proof of Concept\n",
    "### Revision: 6 | Last Updated: 2025-07-14 13:54\n",
    "\n",
    "This notebook demonstrates a simple proof of concept for fake review detection using a pretrained transformer model. We'll use a small dataset of reviews to showcase the basic functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies\n",
    "\n",
    "This notebook is designed to run on SageMaker Distribution 3.2.0, which comes with most required packages pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verify NLTK resources are available\n",
    "import nltk\n",
    "\n",
    "# Check if resources exist, download only if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    print(\"NLTK punkt tokenizer is available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt tokenizer...\")\n",
    "    nltk.download('punkt')\n",
    "\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"NLTK stopwords corpus is available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK stopwords corpus...\")\n",
    "    nltk.download('stopwords')\n",
    "    \n",
    "# Download punkt_tab resource which is specifically needed for tokenization\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt_tab')\n",
    "    print(\"NLTK punkt_tab resource is available\")\n",
    "except LookupError:\n",
    "    print(\"Downloading NLTK punkt_tab resource...\")\n",
    "    nltk.download('punkt_tab')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Check transformers version\n",
    "import transformers\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "\n",
    "# Add the src directory to the path\n",
    "# Get the absolute path to ensure imports work correctly\n",
    "notebook_dir = os.path.abspath('')\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Verify the path is correct\n",
    "print(f\"Project root directory: {project_root}\")\n",
    "print(f\"Files in project root: {os.listdir(project_root)}\")\n",
    "\n",
    "# Import project modules\n",
    "from src.preprocessing import preprocess_text\n",
    "from src.model import load_pretrained_model, predict\n",
    "from src.evaluation import evaluate_model, plot_confusion_matrix"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data\n",
    "\n",
    "For this PoC, we'll create a small dataset of reviews. We'll avoid using the amazon_reviews_multi dataset directly due to potential checksum issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Option 1: Use a different dataset that's less likely to have checksum issues\n",
    "try:\n",
    "    # Try loading IMDB dataset (more stable and commonly used)\n",
    "    dataset = load_dataset(\"imdb\", split=\"train[:1000]\")\n",
    "    print(\"Successfully loaded IMDB dataset\")\n",
    "    \n",
    "    # Map the IMDB dataset structure to match our expected format\n",
    "    # IMDB has 'text' and 'label' fields (0=negative, 1=positive)\n",
    "    # We'll map this to 'review_body' and 'label' (0=genuine, 1=fake)\n",
    "    # For demonstration, we'll consider positive reviews as genuine and negative as fake\n",
    "    dataset = dataset.rename_column(\"text\", \"review_body\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading IMDB dataset: {e}\")\n",
    "    print(\"Falling back to creating a sample dataset manually\")\n",
    "    \n",
    "    # Option 2: Create a small sample dataset manually\n",
    "    reviews = [\n",
    "        {\"review_body\": \"This product is amazing! I've been using it for a month and it has completely changed my life. The quality is outstanding and it's worth every penny.\", \"label\": 0},\n",
    "        {\"review_body\": \"I bought this yesterday and it's already broken. Terrible quality and customer service didn't help at all. Complete waste of money.\", \"label\": 1},\n",
    "        {\"review_body\": \"Best purchase ever!!! Five stars!!! Amazing product!!! Buy it now!!! You won't regret it!!!\", \"label\": 1},\n",
    "        {\"review_body\": \"The product arrived on time and works as expected. Good value for the price. I would recommend it to others looking for a budget option.\", \"label\": 0},\n",
    "        {\"review_body\": \"I've had this product for about 6 months now and it's holding up well. No complaints and does exactly what it's supposed to do.\", \"label\": 0},\n",
    "        {\"review_body\": \"DO NOT BUY THIS!!! It's a complete scam! The seller is dishonest and the product is nothing like described!!!\", \"label\": 1},\n",
    "        {\"review_body\": \"Average product, nothing special but gets the job done. Packaging was nice and delivery was quick.\", \"label\": 0},\n",
    "        {\"review_body\": \"I was skeptical at first but this product exceeded my expectations. The design is elegant and functionality is top-notch.\", \"label\": 0},\n",
    "        {\"review_body\": \"This changed my life!!! I can't believe how amazing this is!!! Everyone needs to buy this right now!!!\", \"label\": 1},\n",
    "        {\"review_body\": \"Product arrived damaged and when I tried to return it, customer service was unhelpful. Very disappointed with this purchase.\", \"label\": 1}\n",
    "    ]\n",
    "    \n",
    "    # Create more synthetic examples\n",
    "    import random\n",
    "    genuine_phrases = [\n",
    "        \"works well\", \"good quality\", \"as described\", \"reasonable price\", \"satisfied with purchase\",\n",
    "        \"would recommend\", \"good value\", \"fast shipping\", \"easy to use\", \"well made\"\n",
    "    ]\n",
    "    \n",
    "    fake_phrases = [\n",
    "        \"amazing!!!\", \"life changing!!!\", \"best ever!!!\", \"miracle product!!!\", \"can't believe it!!!\",\n",
    "        \"worst product ever\", \"complete scam\", \"total ripoff\", \"don't waste your money\", \"absolutely terrible\"\n",
    "    ]\n",
    "    \n",
    "    # Generate additional examples\n",
    "    for _ in range(90):\n",
    "        # Generate genuine reviews (with 0-2 exclamation points max)\n",
    "        genuine = \"I purchased this product last month. \" + random.choice(genuine_phrases).capitalize() + \\\n",
    "                 \". \" + random.choice(genuine_phrases).capitalize() + \\\n",
    "                 \". Overall \" + random.choice(genuine_phrases) + \".\"\n",
    "        genuine = genuine.replace(\"!!!\", \".\") + (\"!\" * random.randint(0, 2))\n",
    "        \n",
    "        # Generate fake reviews (with lots of exclamation points)\n",
    "        fake = random.choice(fake_phrases).upper() + \"!!! \" + \\\n",
    "               random.choice(fake_phrases).capitalize() + \"!!! \" + \\\n",
    "               random.choice(fake_phrases).capitalize() + \"!!!\"\n",
    "        \n",
    "        reviews.append({\"review_body\": genuine, \"label\": 0})\n",
    "        reviews.append({\"review_body\": fake, \"label\": 1})\n",
    "    \n",
    "    # Convert to Dataset\n",
    "    sample_df = pd.DataFrame(reviews)\n",
    "    dataset = Dataset.from_pandas(sample_df)\n",
    "\n",
    "# Display dataset info\n",
    "dataset"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Convert to pandas for easier manipulation\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Check class distribution\n",
    "print(\"Class distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Preprocess the review text\n",
    "df['processed_review'] = df['review_body'].apply(preprocess_text)\n",
    "\n",
    "# Split into train and test sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}\")\n",
    "print(f\"Test set size: {len(test_df)}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Pretrained Model\n",
    "\n",
    "We'll use a lightweight pretrained model for this PoC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Load pretrained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "model, tokenizer = load_pretrained_model(model_name, num_labels=2)\n",
    "\n",
    "print(f\"Loaded model: {model_name}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Tokenize the datasets\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples, padding=\"max_length\", truncation=True, max_length=128)\n",
    "\n",
    "# Prepare training dataset\n",
    "train_texts = train_df['processed_review'].tolist()\n",
    "train_labels = train_df['label'].tolist()\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "\n",
    "# Prepare test dataset\n",
    "test_texts = test_df['processed_review'].tolist()\n",
    "test_labels = test_df['label'].tolist()\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "class ReviewDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewDataset(test_encodings, test_labels)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Fine-tune the Model\n",
    "\n",
    "We'll fine-tune the pretrained model on our dataset. This section is split into smaller steps to help isolate any issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5.1: Set up directories\n",
    "# Use absolute paths to ensure files are saved in the correct location\n",
    "output_dir = os.path.join(project_root, 'models', 'fake_review_detector')\n",
    "logging_dir = os.path.join(project_root, 'results', 'logs')\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "os.makedirs(logging_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Output directory: {output_dir}\")\n",
    "print(f\"Logging directory: {logging_dir}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5.2: Configure training arguments\n",
    "# Using parameters compatible with the installed version of Transformers\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=logging_dir,\n",
    "    logging_steps=10,\n",
    "    # For older versions of Transformers, use eval_strategy instead of evaluation_strategy\n",
    "    eval_strategy=\"steps\",  # Match with save_strategy\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",  # Explicitly set save strategy\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\"  # Specify which metric to use for best model\n",
    ")\n",
    "\n",
    "print(\"Training arguments configured successfully\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5.3: Define evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return evaluate_model(labels, predictions)\n",
    "\n",
    "print(\"Evaluation metrics defined\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5.4: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized successfully\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Step 5.5: Fine-tune the model\n",
    "# This step might be memory-intensive and could cause kernel crashes\n",
    "# Try reducing batch size or model size if it fails\n",
    "try:\n",
    "    print(\"Starting model fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Model fine-tuning completed successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "    print(\"\\nTroubleshooting tips:\")\n",
    "    print(\"1. Try reducing batch size (per_device_train_batch_size)\")\n",
    "    print(\"2. Use a smaller model like 'distilbert-base-uncased'\")\n",
    "    print(\"3. Reduce the maximum sequence length in tokenization\")\n",
    "    print(\"4. Ensure you have enough memory available\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Evaluate the model on the test set\n",
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation results:\")\n",
    "for key, value in eval_results.items():\n",
    "    print(f\"{key}: {value:.4f}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Make predictions on the test set\n",
    "raw_predictions = trainer.predict(test_dataset)\n",
    "y_pred = np.argmax(raw_predictions.predictions, axis=-1)\n",
    "y_true = test_labels\n",
    "\n",
    "# Plot confusion matrix\n",
    "cm_plot = plot_confusion_matrix(y_true, y_pred, labels=['Genuine', 'Fake'])\n",
    "cm_plot.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test with Sample Reviews\n",
    "\n",
    "Let's test our model with some sample reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sample reviews for testing\n",
    "sample_reviews = [\n",
    "    \"This product is amazing! I've been using it for a month and it has completely changed my life. Highly recommend!\",\n",
    "    \"I bought this yesterday and it's already broken. Terrible quality and customer service didn't help at all.\",\n",
    "    \"Best purchase ever!!! Five stars!!! Amazing product!!! Buy it now!!!\",\n",
    "    \"The product arrived on time and works as expected. Good value for the price.\"\n",
    "]\n",
    "\n",
    "# Preprocess the samples\n",
    "processed_samples = [preprocess_text(review) for review in sample_reviews]\n",
    "\n",
    "# Make predictions\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "results = []\n",
    "for i, (original, processed) in enumerate(zip(sample_reviews, processed_samples)):\n",
    "    pred_class, confidence = predict(model, tokenizer, processed, device)\n",
    "    results.append({\n",
    "        \"review\": original,\n",
    "        \"prediction\": \"Fake\" if pred_class == 1 else \"Genuine\",\n",
    "        \"confidence\": confidence\n",
    "    })\n",
    "\n",
    "# Display results\n",
    "for i, result in enumerate(results):\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Review: {result['review']}\")\n",
    "    print(f\"Prediction: {result['prediction']} (Confidence: {result['confidence']:.4f})\")\n",
    "    print(\"---\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save the Model for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Save the model and tokenizer\n",
    "model_save_path = os.path.join(project_root, 'models', 'fake_review_detector_final')\n",
    "os.makedirs(model_save_path, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model and tokenizer saved to {model_save_path}\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Next Steps for Multi-lingual Capabilities\n",
    "\n",
    "This notebook demonstrates a basic proof of concept for fake review detection. To expand to multi-lingual capabilities, consider these next steps:\n",
    "\n",
    "1. **Use a multi-lingual model**: Replace DistilBERT with XLM-RoBERTa or mBERT\n",
    "   ```python\n",
    "   model_name = \"xlm-roberta-base\"  # Supports 100+ languages\n",
    "   # or\n",
    "   model_name = \"bert-base-multilingual-cased\"  # Supports 104 languages\n",
    "   ```\n",
    "\n",
    "2. **Collect multi-lingual training data**: Gather labeled fake/genuine reviews in multiple languages\n",
    "\n",
    "3. **Evaluate per language**: Set up separate evaluation for each language to ensure consistent performance\n",
    "\n",
    "4. **Consider language-specific fine-tuning**: Fine-tune on each language separately or use a mixed approach\n",
    "\n",
    "5. **Implement language detection**: Add a preprocessing step to detect the language of incoming reviews\n",
    "\n",
    "6. **Deploy as a SageMaker endpoint**: Create a real-time inference endpoint for production use\n",
    "\n",
    "7. **Implement monitoring**: Set up model monitoring to detect performance drift over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
